{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN\n",
    "\n",
    "Vary the data used to train different models with noisy or coarse d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import xarray as xr\n",
    "import pickle\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2aab5a4e69d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0) # for reproduceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zc():\n",
    "    return np.array([-481.68085,-446.76865,-414.0377,-383.35178,-354.58304,-327.6118,-302.32565,-278.61935,\n",
    "                           -256.39423,-235.5577,-216.02301,-197.70883,-180.5389,-164.44174,-149.3503,-135.20177,\n",
    "                           -121.937225,-109.50143,-97.84261,-86.91223,-76.664764,-67.05755,-58.050587,-49.60637,\n",
    "                           -41.689735,-34.267727,-27.309437,-20.785896,-14.669938,-8.9361,-3.5605056,-0.5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class psom_data(Dataset):\n",
    "    \"\"\"creates dataset from model outputs for training based on list of simulations to use and\n",
    "       range of days to include\"\"\"\n",
    "\n",
    "    def __init__(self, res=1, datadir='data', noise = 0, days=(5,10), var = (0,1,3,5,6), transform=None, standardize=True, basic=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            datadir (string): path to model outputs\n",
    "            day (int): which day of analysis period to use (start_day + day)\n",
    "            var (tuple): indices of variables to use\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"     \n",
    "        X = []; Y = []; center = []; exp = []; day_list = [];\n",
    "        i1 = []; i2 = []; j1 = []; j2 = []\n",
    "        \n",
    "        for day in days:\n",
    "            with open(os.path.join(datadir,'coarse_%ikm_day%i.pkl'%(res,day)),'rb') as f:\n",
    "                X_temp, Y_temp, center_temp, coord_temp, exp_temp = pickle.load(f)\n",
    "        \n",
    "            X.extend(X_temp); Y.extend(Y_temp); center.extend(center_temp); \n",
    "            i1.extend(coord_temp[0]); i2.extend(coord_temp[1]);\n",
    "            j1.extend(coord_temp[2]); j2.extend(coord_temp[3]);\n",
    "            exp.extend(exp_temp); day_list.extend([day]*len(X_temp))\n",
    "                \n",
    "        if standardize:\n",
    "            Xarr = np.array(X)\n",
    "            mean = np.mean(Xarr, axis=(0,2,3), keepdims=True)\n",
    "            std = np.std(Xarr, axis=(0,2,3), keepdims=True)\n",
    "            X = (Xarr-mean)/std\n",
    "            \n",
    "        self.X = X # list of inputs, each sample has dim (N_var, n, n)\n",
    "        self.Y = Y # list of outputs, each sample has dim (depth,)\n",
    "        self.center = center\n",
    "        self.i1 = i1\n",
    "        self.i2 = i2\n",
    "        self.j1 = j1\n",
    "        self.j2 = j2\n",
    "        self.exp = exp\n",
    "        self.transform = transform\n",
    "        self.basic=basic\n",
    "        self.day = day_list\n",
    "        self.var = var\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.X)==len(self.Y), 'X and Y must have the same lengths'\n",
    "        assert len(self.exp)==len(self.i1), 'coordinates and experiments must have same lengths'\n",
    "        assert len(self.i1)==len(self.i2)==len(self.j1)==len(self.j2), 'coordinates must have same lengths'\n",
    "        return len(self.X)  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "            \n",
    "        sample = {'X': self.X[idx][self.var,:,:], 'Y': self.Y[idx], 'center': self.center[idx],\n",
    "          'coord1': (self.j1[idx], self.i1[idx]), 'coord2': (self.j2[idx], self.i2[idx]), 'exp': self.exp[idx]}\n",
    "            \n",
    "        if self.basic:\n",
    "            return sample['X'],sample['Y']\n",
    "        else:\n",
    "            return sample['X'], sample['Y'], sample['center'], sample['coord1'], sample['coord2'], sample['exp']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data and split into training/validation/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, valid_size):\n",
    "    \"\"\"\n",
    "    Randomly split the data into a training and validation set. Returns the a set of samples for training and validation.\n",
    "    \"\"\"    \n",
    "    #dataset = psom_data(datadir=datadir, days=days, basic=True, standardize=True)\n",
    "\n",
    "    data_idx = [i for i in range(len(dataset))]\n",
    "        \n",
    "    # shuffling the indices and adding them to the current training and testing sets\n",
    "    np.random.shuffle(data_idx)\n",
    "    split = int(np.floor(valid_size * len(data_idx)))\n",
    "        \n",
    "    train_idx = data_idx[split:]\n",
    "    test_idx = data_idx[:split]\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_idx)\n",
    "    test_sampler = SubsetRandomSampler(test_idx)\n",
    "    \n",
    "    return train_sampler, test_sampler\n",
    "\n",
    "def load_data(datadir = 'data', days = [5], res = 1, noise = 0, var=(0,1,3,5,6), valid_size = .2, batchsize=1):\n",
    "    \"\"\"\n",
    "    Randomly split the data into a training and validation set. Returns the a set of samples for training and validation.\n",
    "    \"\"\"\n",
    "    print('reading in data...')\n",
    "    dataset = psom_data(datadir=datadir, res = res, noise = noise, days=days, var=var, basic=True, standardize=True)\n",
    "    \n",
    "    print('shuffling data...')\n",
    "    train_sampler, test_sampler = split_data(dataset, valid_size)\n",
    "    \n",
    "    print('loading train and test loader...')\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, \n",
    "                                               sampler=train_sampler)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(dataset, batch_size=batchsize, \n",
    "                                               sampler=test_sampler)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with 3 convolutional layers and 2 fully connected layers\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # layer 1 consists of conv, relu, MP\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(5, 16, kernel_size=5, padding=0), nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        # layer 2 consists of conv, relu, MP\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(16, 32, kernel_size=3, padding=0), nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "        # layer 3 consists of conv, relu, MP\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(32, 64, kernel_size=3, padding=0), nn.ReLU(inplace=True),\n",
    "                                   nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.fc1 = nn.Sequential(nn.Linear(256, 64), nn.ReLU(inplace=True))\n",
    "        self.fc2 = nn.Sequential(nn.Linear(64, 32)) # output is a profile of w\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = x.view(-1, 256)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "def train(model, train_loader,batchsize=1,lr=0.01):\n",
    "    model.train()\n",
    "    i=0\n",
    "    avg_loss = 0\n",
    "    for X,Y in train_loader:\n",
    "        i+=1\n",
    "        optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "        optimizer.zero_grad()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        tensor = X.view(X.shape[0],X.shape[1],X.shape[2],X.shape[3]).float()\n",
    "        tensor = tensor.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        output = model(tensor)\n",
    "        loss = F.mse_loss(output, Y.float())\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return avg_loss/(len(train_loader))\n",
    "\n",
    "def test(model, test_loader, batchsize=1):\n",
    "    model.eval()\n",
    "    \n",
    "    loss = 0\n",
    "    Lprofile = np.zeros(32)\n",
    "    w_mag = np.zeros(32)\n",
    "    \n",
    "    for X,Y in test_loader:\n",
    "        tensor = X.view(X.shape[0],X.shape[1],X.shape[2],X.shape[3]).float()\n",
    "        tensor = tensor.to(device)\n",
    "        Y = Y.to(device)\n",
    "        pred = model(tensor)\n",
    "        \n",
    "        loss += F.mse_loss(pred, Y.float()).item()\n",
    "        \n",
    "        # calculate MSE at each depth\n",
    "        Lprofile += np.sum((pred.cpu().detach().numpy()-Y.cpu().detach().numpy())**2, axis=0).flatten()\n",
    "        \n",
    "        # calculate magnitude of w at each depth\n",
    "        w_mag += np.sum((Y.cpu().detach().numpy())**2, axis=0).flatten()\n",
    "        \n",
    "        #print(loss)\n",
    "    mean_Lprofile = Lprofile/len(test_loader)\n",
    "    rms = (mean_Lprofile)**0.5\n",
    "    w_rms = (w_mag/len(test_loader))**0.5\n",
    "    \n",
    "    return loss/len(test_loader), rms, w_rms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model, res = 10km, noise=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n",
      "shuffling data...\n",
      "loading train and test loader...\n",
      "238032\n"
     ]
    }
   ],
   "source": [
    "trainloader, testloader_null = load_data(datadir = 'data/coarse/', res = 10, days = [0,10,15,20], var = (0,1,3,5,6), batchsize=1,valid_size = 0)\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n",
      "shuffling data...\n",
      "loading train and test loader...\n",
      "reading in data...\n",
      "shuffling data...\n",
      "loading train and test loader...\n",
      "time to load data: 1.25 min\n",
      "epoch 0\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0567\n",
      "mse test loss: 0.0461\n",
      "epoch 1\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0535\n",
      "mse test loss: 0.0433\n",
      "epoch 2\n",
      "time to train epoch: 1.48 min\n",
      "mse train loss: 0.0505\n",
      "mse test loss: 0.0412\n",
      "epoch 3\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0484\n",
      "mse test loss: 0.0397\n",
      "epoch 4\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0460\n",
      "mse test loss: 0.0382\n",
      "epoch 5\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0439\n",
      "mse test loss: 0.0371\n",
      "epoch 6\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0422\n",
      "mse test loss: 0.0365\n",
      "epoch 7\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0407\n",
      "mse test loss: 0.0356\n",
      "epoch 8\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0391\n",
      "mse test loss: 0.0345\n",
      "epoch 9\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0373\n",
      "mse test loss: 0.0334\n",
      "epoch 10\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0350\n",
      "mse test loss: 0.0324\n",
      "epoch 11\n",
      "time to train epoch: 1.49 min\n",
      "mse train loss: 0.0328\n",
      "mse test loss: 0.0314\n",
      "epoch 12\n",
      "time to train epoch: 1.50 min\n",
      "mse train loss: 0.0309\n",
      "mse test loss: 0.0307\n",
      "epoch 13\n",
      "time to train epoch: 1.52 min\n",
      "mse train loss: 0.0295\n",
      "mse test loss: 0.0302\n",
      "epoch 14\n",
      "time to train epoch: 1.50 min\n",
      "mse train loss: 0.0283\n",
      "mse test loss: 0.0297\n",
      "epoch 15\n",
      "time to train epoch: 1.57 min\n",
      "mse train loss: 0.0272\n",
      "mse test loss: 0.0294\n",
      "epoch 16\n",
      "time to train epoch: 1.58 min\n",
      "mse train loss: 0.0263\n",
      "mse test loss: 0.0288\n",
      "epoch 17\n",
      "time to train epoch: 1.51 min\n",
      "mse train loss: 0.0255\n",
      "mse test loss: 0.0284\n",
      "epoch 18\n",
      "time to train epoch: 1.51 min\n",
      "mse train loss: 0.0247\n",
      "mse test loss: 0.0281\n",
      "epoch 19\n",
      "time to train epoch: 1.51 min\n",
      "mse train loss: 0.0240\n",
      "mse test loss: 0.0280\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/v3_coarse15km_1e-3/trained_cnn.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 41>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse test loss: \u001b[39m\u001b[38;5;132;01m%0.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39mtloss)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Save\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/trained_cnn.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m np\u001b[38;5;241m.\u001b[39msave(model_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/train_loss.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(loss_epochs))\n\u001b[1;32m     43\u001b[0m np\u001b[38;5;241m.\u001b[39msave(model_dir\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/test_loss.npy\u001b[39m\u001b[38;5;124m'\u001b[39m,np\u001b[38;5;241m.\u001b[39marray(test_loss))\n",
      "File \u001b[0;32m~/.conda/envs/cnn/lib/python3.9/site-packages/torch/serialization.py:377\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m\"\"\"save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mSaves an object to a disk file.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    >>> torch.save(x, buffer)\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    375\u001b[0m _check_dill_version(pickle_module)\n\u001b[0;32m--> 377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m    379\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n",
      "File \u001b[0;32m~/.conda/envs/cnn/lib/python3.9/site-packages/torch/serialization.py:231\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/.conda/envs/cnn/lib/python3.9/site-packages/torch/serialization.py:212\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/v3_coarse15km_1e-3/trained_cnn.pt'"
     ]
    }
   ],
   "source": [
    "# vary these--------\n",
    "res = 15\n",
    "batch_size=8\n",
    "noise_level = 0\n",
    "epochs = 20\n",
    "learning_rate = 1e-3\n",
    "model_dir = 'models/v3_coarse15km_1e-3'\n",
    "#--------------------\n",
    "\n",
    "# load data\n",
    "s0 = time.time()\n",
    "trainloader, testloader_null = load_data(datadir = 'data/coarse', res = res, noise = noise_level, days = [0,10,15,20], var = (0,1,3,5,6), batchsize=batch_size,valid_size = 0)\n",
    "del testloader_null\n",
    "trainloader_null, testloader = load_data(datadir = 'data/coarse', res = res, noise = noise_level, days = [5], var=(0,1,3,5,6), batchsize=batch_size,valid_size = 1)\n",
    "del trainloader_null\n",
    "print('time to load data: %0.2f min' %((time.time()-s0)/60))\n",
    "\n",
    "# Train\n",
    "net = Net()\n",
    "net.to(device)\n",
    "\n",
    "loss_epochs=[]\n",
    "test_loss=[]\n",
    "\n",
    "for i in range(epochs):   \n",
    "    print('epoch %i'%i)\n",
    "    \n",
    "    # train\n",
    "    s1 = time.time()\n",
    "    loss = train(net, trainloader, batchsize=batch_size, lr=learning_rate)\n",
    "    loss_epochs.append(loss)\n",
    "    print('time to train epoch: %0.2f min' %((time.time()-s1)/60))\n",
    "    \n",
    "    print('mse train loss: %0.4f' %loss)\n",
    "    tloss, rms, w_rms = test(net, testloader)\n",
    "    test_loss.append(tloss)\n",
    "    print('mse test loss: %0.4f' %tloss)\n",
    "    \n",
    "# Save\n",
    "\n",
    "torch.save(net.state_dict(), model_dir+'/trained_cnn.pt')\n",
    "np.save(model_dir+'/train_loss.npy',np.array(loss_epochs))\n",
    "np.save(model_dir+'/test_loss.npy',np.array(test_loss))\n",
    "np.save(model_dir+'/test_wrms_profile.npy',np.array(w_rms))\n",
    "np.save(model_dir+'/test_rms_profile.npy',np.array(rms))\n",
    "\n",
    "# Plot\n",
    "plt.plot(loss_epochs, 'o-',label='train loss')\n",
    "plt.plot(test_loss, 'o-', label='test loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('mse loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
